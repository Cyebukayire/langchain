{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd3aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain is a very imporntant concept in LLM\n",
    "# You can chain different LLMs together, \n",
    "# The output from one LLM can be an input for another LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2658fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory, CombinedMemory, ConversationSummaryMemory\n",
    "import os\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0fe5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c38d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1dd43d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = \"What is a good name for a company that makes {product}?\",\n",
    "    input_variables = [\"product\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "chat = ChatOpenAI(temperature = 0.9)\n",
    "chain = LLMChain(llm = chat, prompt = chat_prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f3330d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TechCore Computing Inc.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"computers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37eb0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f505cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE A CATCHPHRASE FOR THIS GENERATED COMPANY NAME FROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e131d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0a8bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine a certain number of conversations we need to keep in our chain\n",
    "conv_memory = ConversationBufferWindowMemory(\n",
    "memory_key = \"chat_history_lines\",\n",
    "input_key = \"input\",\n",
    "k = 1 # Include only the latest conversations in the prompt\n",
    ")\n",
    "\n",
    "# Summarise all previous conversations (Helps save a number of tokens)\n",
    "summary_memory = ConversationSummaryMemory(llm = OpenAI(), input_key = \"input\")\n",
    "memory = CombinedMemory(memories = [conv_memory, summary_memory])\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "input_variables = ['history', 'input', 'chat_history_lines'],\n",
    "template = \"\"\"The following is a conversation between a human and AI\n",
    "Summary of conversation:\n",
    "{history}\n",
    "Current conversation:\n",
    "{chat_history_lines}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature = 0)\n",
    "conversation = ConversationChain(\n",
    "llm = llm,\n",
    "    verbose = True,\n",
    "    memory = memory,\n",
    "    prompt = PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7249554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = PromptTemplate (\n",
    "    input_variables = [\"company_name\"],\n",
    "    template = \"Wrie a catchphrase for the following company: {company_name}\"\n",
    ")\n",
    "chain_two = LLMChain(llm = llm, prompt = second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe77cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains = [chain, chain_two], verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f99064eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain specifying only the input variable for the first chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e984be03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mTechSurge Solutions.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "\"Unlock the power of TechSurge Solutions - Innovate, Automate, Accelerate!\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\"Unlock the power of TechSurge Solutions - Innovate, Automate, Accelerate!\"\n"
     ]
    }
   ],
   "source": [
    "catchphrase = overall_chain.run('TechWave')\n",
    "print(catchphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71be515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
