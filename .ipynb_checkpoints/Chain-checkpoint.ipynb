{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dd3aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain is a very imporntant concept in LLM\n",
    "# You can chain different LLMs together, \n",
    "# The output from one LLM can be an input for another LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2658fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory, CombinedMemory, ConversationSummaryMemory\n",
    "import os\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c38d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1dd43d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = \"What is a good name for a company that makes {product}?\",\n",
    "    input_variables = [\"product\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "chat = ChatOpenAI(temperature = 0.9)\n",
    "chain = LLMChain(llm = chat, prompt = chat_prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64f3330d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a63b19f374248c1f1c9be0985866d490 in your message.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TechWave\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"computers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37eb0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f505cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE A COMPANY NAME FOR THIS GENERATED COMPANY NAME FROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e131d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0a8bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine a certain number of conversations we need to keep in our chain\n",
    "conv_memory = ConversationBufferWindowMemory(\n",
    "memory_key = \"chat_history_lines\",\n",
    "input_key = \"input\",\n",
    "k = 1 # Include only the latest conversations in the prompt\n",
    ")\n",
    "\n",
    "# Summarise all previous conversations (Helps save a number of tokens)\n",
    "summary_memory = ConversationSummaryMemory(llm = OpenAI(), input_key = \"input\")\n",
    "memory = CombinedMemory(memories = [conv_memory, summary_memory])\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "input_variables = ['history', 'input', 'chat_history_lines'],\n",
    "template = \"\"\"The following is a conversation between a human and AI\n",
    "Summary of conversation:\n",
    "{history}\n",
    "Current conversation:\n",
    "{chat_history_lines}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature = 0)\n",
    "conversation = ConversationChain(\n",
    "llm = llm,\n",
    "    verbose = True,\n",
    "    memory = memory,\n",
    "    prompt = PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7249554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = PromptTemplate (\n",
    "    input_variables = [\"company_name\"],\n",
    "    template = \"Wrie a catchphrase for the following company: {company_name}\"\n",
    ")\n",
    "chain_two = LLMChain(llm = llm, prompt = second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe77cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains = [chain, chain_two], verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f99064eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tthe chain specifying only the input variable for the first chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984be03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
